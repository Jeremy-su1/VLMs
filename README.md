# Visionâ€“Language Modeling (VLM) Lecture Notebooks

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/Jeremy-su1/VLMs/blob/main/1_CLIP_ZeroShot.ipynb)  
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/Jeremy-su1/VLMs/blob/main/2_CLIP_GradCAM.ipynb)

A hands-on multi-modal CLIP mini-course on **Google Colab**. No local setupâ€”just click â€œOpen in Colabâ€ and run!

---

## ğŸ“š Notebooks

1. **`1_CLIP_ZeroShot.ipynb`**  
   - Installs required packages right in Colab  
   - Loads OpenAI CLIP  
   - Builds zero-shot classifiers in one cell  
   - Exercises: tweak prompts, try your own images

2. **`2_CLIP_GradCAM.ipynb`**  
   - Runs Grad-CAM on CLIPâ€™s image encoder  
   - Generates activation heatmaps for any text query  
   - Exercises: visualize on custom uploads

---

## ğŸš€ Quick Start (Colab)

1. **Open** one of the badges above (or paste the notebook URL into Colab).  
2. **Runtime â–¶ï¸ Run all** â€” every dependency installs automatically.  
3. **Upload** your images via the left-pane â€œFilesâ€ tab or mount Drive.  
4. **Modify & explore!** Change prompts, visualize your own data, extend code.

---

## ğŸ¯ Goals

- Grasp CLIPâ€™s contrastive training paradigm  
- Create zero-shot classifiers for arbitrary concepts  
- Visualize model focus with Grad-CAM heatmaps  
- Analyze embeddings for retrieval & clustering  
- Kickstart your own vision-language research
