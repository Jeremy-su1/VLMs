# Vision–Language Modeling (VLM) Lecture Notebooks

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/Jeremy-su1/VLMs/blob/main/1_CLIP_ZeroShot.ipynb)  
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/Jeremy-su1/VLMs/blob/main/2_CLIP_GradCAM.ipynb)

A hands-on multi-modal CLIP mini-course on **Google Colab**. No local setup—just click “Open in Colab” and run!

---

## 📚 Notebooks

1. **`1_CLIP_ZeroShot.ipynb`**  
   - Installs required packages right in Colab  
   - Loads OpenAI CLIP  
   - Builds zero-shot classifiers in one cell  
   - Exercises: tweak prompts, try your own images

2. **`2_CLIP_GradCAM.ipynb`**  
   - Runs Grad-CAM on CLIP’s image encoder  
   - Generates activation heatmaps for any text query  
   - Exercises: visualize on custom uploads

---

## 🚀 Quick Start (Colab)

1. **Open** one of the badges above (or paste the notebook URL into Colab).  
2. **Runtime ▶︎ Run all** — every dependency installs automatically.  
3. **Upload** your images via the left-pane “Files” tab or mount Drive.  
4. **Modify & explore!** Change prompts, visualize your own data, extend code.

---

## 🎯 Goals

- Grasp CLIP’s contrastive training paradigm  
- Create zero-shot classifiers for arbitrary concepts  
- Visualize model focus with Grad-CAM heatmaps  
- Analyze embeddings for retrieval & clustering  
- Kickstart your own vision-language research
